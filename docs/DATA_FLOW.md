# Observability Data Flow: Metrics, Logs, and Traces

This document explains how telemetry data flows from your services to Grafana. **There is no single shared collector** - each type of observability data (metrics, logs, traces) has its own specialized pipeline optimized for its unique characteristics.

## Overview Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Microservice (Any of the 3)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Metrics    â”‚  â”‚     Logs     â”‚  â”‚   Traces       â”‚          â”‚
â”‚  â”‚  (Prometheus â”‚  â”‚  (Structured â”‚  â”‚ (OpenTelemetry â”‚          â”‚
â”‚  â”‚   client)    â”‚  â”‚     JSON)    â”‚  â”‚    SDK)        â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â”‚                 â”‚                  â”‚                   â”‚
â”‚    /metrics              stdout            OTLP gRPC             â”‚
â”‚   endpoint              (JSON logs)        :4317                 â”‚
â”‚         â”‚                 â”‚                  â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                  â”‚
          â”‚ PULL            â”‚ PUSH             â”‚ PUSH
          â”‚ (scrape)        â”‚ (stream)         â”‚ (export)
          â”‚                 â”‚                  â”‚
          â–¼                 â–¼                  â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Prometheus  â”‚   â”‚  Promtail   â”‚   â”‚    Tempo    â”‚
   â”‚             â”‚   â”‚ (DaemonSet) â”‚   â”‚             â”‚
   â”‚ Scrapes     â”‚   â”‚ Collects    â”‚   â”‚ OTLP        â”‚
   â”‚ every 30s   â”‚   â”‚ container   â”‚   â”‚ Receiver    â”‚
   â”‚             â”‚   â”‚ logs        â”‚   â”‚             â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                 â”‚
         â”‚ Stores          â”‚ Ships           â”‚ Stores
         â”‚ time-series     â”‚ to Loki         â”‚ trace spans
         â”‚                 â”‚                 â”‚
         â–¼                 â–¼                 â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Prometheus  â”‚   â”‚    Loki     â”‚   â”‚    Tempo    â”‚
   â”‚  Storage    â”‚   â”‚  Storage    â”‚   â”‚  Storage    â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                 â”‚
         â”‚                 â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ Query
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Grafana   â”‚
                    â”‚             â”‚
                    â”‚ - Dashboardsâ”‚
                    â”‚ - Explore   â”‚
                    â”‚ - Alerts    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Metrics Flow (PULL Model)

### Architecture

```
Service exposes /metrics endpoint
         â†“
   [Prometheus]
   Scrapes every 30 seconds via HTTP GET
         â†“
   Stores as time-series data
         â†“
   [Grafana]
   Queries Prometheus using PromQL
```

### How It Works in Our Demo

**Service Side** - Expose metrics endpoint:

```python
# Python Service Example
from prometheus_client import Counter, generate_latest, CONTENT_TYPE_LATEST
from fastapi import Response

# Define metric once
requests_total = Counter('http_requests_total', 'Total requests', 
                         ['method', 'endpoint', 'status'])

# Increment in your code
@app.post("/api/users")
async def create_user():
    # ... business logic ...
    requests_total.labels(method="POST", endpoint="/api/users", status=201).inc()
    return user

# Expose metrics endpoint
@app.get("/metrics")
async def metrics():
    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

**Prometheus Side** - Scrape configuration:

```yaml
# prometheus-config.yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'user-service'
    static_configs:
      - targets: ['python-user-service:8000']
    metrics_path: '/metrics'
  
  - job_name: 'order-service'
    static_configs:
      - targets: ['rust-order-service:8001']
    
  - job_name: 'inventory-service'
    static_configs:
      - targets: ['go-inventory-service:8002']
```

**Kubernetes ServiceMonitor** (auto-discovery):

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: python-user-service
spec:
  selector:
    matchLabels:
      app: python-user-service
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
```

### Key Characteristics

- âœ… **PULL model**: Prometheus actively scrapes services
- âœ… **HTTP endpoint**: Services expose `/metrics` on demand
- âœ… **Scrape interval**: Configurable (default 15-30 seconds)
- âœ… **Text format**: Plain text Prometheus format
- âœ… **Service discovery**: Kubernetes ServiceMonitors auto-discover services
- âœ… **No agent needed**: Services only need to expose endpoint
- âœ… **Stateless**: Services don't need to push or queue data

### What Gets Collected

Example metrics output from `/metrics`:

```
# HELP http_requests_total Total HTTP requests
# TYPE http_requests_total counter
http_requests_total{method="GET",endpoint="/api/users",status="200"} 1543
http_requests_total{method="POST",endpoint="/api/users",status="201"} 89

# HELP http_request_duration_seconds HTTP request duration
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{le="0.1"} 1200
http_request_duration_seconds_bucket{le="0.5"} 1500
http_request_duration_seconds_sum 450.5
http_request_duration_seconds_count 1632
```

---

## 2. Logs Flow (PUSH Model)

### Architecture

```
Service writes logs to stdout/stderr
         â†“
   Container runtime captures logs
         â†“
   [Promtail DaemonSet]
   Runs on every K8s node
   Reads container logs from /var/log/pods
         â†“
   Adds labels (namespace, pod, container)
         â†“
   [Loki]
   Receives and indexes logs
   Stores with labels (not full-text indexing!)
         â†“
   [Grafana]
   Queries Loki using LogQL
```

### How It Works in Our Demo

**Service Side** - Just log to stdout:

```python
# Python Service - No special code needed!
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Structured JSON logging (better for querying)
logger.info("User created", extra={
    "user_id": 123,
    "email": "user@example.com",
    "trace_id": "abc123"
})

# Output:
# {"timestamp":"2025-12-19T10:30:45","level":"info","msg":"User created",
#  "user_id":123,"email":"user@example.com","trace_id":"abc123"}
```

```rust
// Rust Service - Logs automatically JSON formatted
use tracing::{info, error};

#[instrument]
async fn create_order(payload: CreateOrderRequest) {
    info!(user_id = payload.user_id, "Creating order");
    // Logs output in JSON format automatically
}

// Output:
// {"timestamp":"2025-12-19T10:30:45","level":"info","target":"order_service",
//  "span":{"name":"create_order"},"fields":{"user_id":123},"msg":"Creating order"}
```

```go
// Go Service - Standard logging
log.Printf("Creating inventory item: %s (SKU: %s)", req.ProductName, req.SKU)

// Output:
// 2025/12/19 10:30:45 Creating inventory item: Gaming Mouse (SKU: MOUSE-001)
```

**Promtail Side** - Automatic collection:

```yaml
# promtail-config.yaml
server:
  http_listen_port: 9080

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: kubernetes-pods
    kubernetes_sd_configs:
      - role: pod
  
    relabel_configs:
      # Extract namespace
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
    
      # Extract pod name
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod
    
      # Extract container name
      - source_labels: [__meta_kubernetes_pod_container_name]
        target_label: container
    
      # Extract app label
      - source_labels: [__meta_kubernetes_pod_label_app]
        target_label: app
```

**Loki Side** - Storage and indexing:

Loki indexes **only labels** (not log content), making it cost-effective:

- `namespace=demo`
- `pod=python-user-service-abc123`
- `container=user-service`
- `app=python-user-service`

The actual log content is compressed and stored without full-text indexing.

### Key Characteristics

- âœ… **PUSH model**: Promtail pushes logs to Loki
- âœ… **Stdout/stderr**: Services just log normally (12-factor app pattern)
- âœ… **DaemonSet**: One Promtail pod per Kubernetes node
- âœ… **Automatic labels**: Metadata extracted from Kubernetes automatically
- âœ… **Structured JSON**: Easy to query and filter
- âœ… **No agent in container**: Promtail reads from host filesystem
- âœ… **Cost-effective**: Only labels indexed (not full log content)

### Querying in Grafana

```logql
# All logs from user service
{app="python-user-service"}

# Logs containing "error"
{app="python-user-service"} |= "error"

# Parse JSON and filter
{app="python-user-service"} | json | user_id="123"

# Logs with specific trace ID
{namespace="demo"} | json | trace_id="abc123"
```

---

## 3. Traces Flow (PUSH Model via OTLP)

### Architecture

```
Service creates spans via OpenTelemetry SDK
         â†“
   [OpenTelemetry SDK]
   Batches spans in memory
         â†“
   Exports via OTLP protocol (gRPC or HTTP)
   to OTEL_EXPORTER_OTLP_ENDPOINT
         â†“
   [Tempo]
   Receives spans on port 4317 (gRPC) or 4318 (HTTP)
   Links spans by trace_id
   Stores complete traces
         â†“
   [Grafana]
   Queries Tempo by trace ID or search criteria
```

### How It Works in Our Demo

**Python** (auto-instrumentation):

```bash
# No code changes needed - just wrap your command!
opentelemetry-instrument \
  --traces_exporter otlp \
  --exporter_otlp_endpoint http://tempo:4317 \
  --service_name user-service \
  uvicorn main:app

# Environment variables work too
export OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4317
export OTEL_SERVICE_NAME=user-service
opentelemetry-instrument uvicorn main:app
```

The wrapper automatically:

- Creates spans for all HTTP requests
- Traces database queries (SQLAlchemy)
- Captures exceptions
- Propagates trace context via headers

**Rust** (manual initialization, then automatic):

```rust
// Initialize once at startup in main.rs
fn init_tracing() {
    let otlp_endpoint = std::env::var("OTEL_EXPORTER_OTLP_ENDPOINT")
        .unwrap_or_else(|_| "http://tempo:4317".to_string());
  
    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(
            opentelemetry_otlp::new_exporter()
                .tonic()
                .with_endpoint(otlp_endpoint)
        )
        .with_trace_config(
            opentelemetry_sdk::trace::config()
                .with_resource(opentelemetry_sdk::Resource::new(vec![
                    opentelemetry::KeyValue::new("service.name", "order-service"),
                ]))
        )
        .install_batch(opentelemetry_sdk::runtime::Tokio)
        .expect("Failed to initialize tracer");

    tracing_subscriber::registry()
        .with(tracing_opentelemetry::layer().with_tracer(tracer))
        .init();
}

// Then just annotate functions - spans created automatically!
#[instrument]
async fn create_order(payload: CreateOrderRequest) -> Result<Order> {
    // Span created automatically for this function
    // All inner function calls also traced
    let result = db.insert_one(order).await?;
    Ok(result)
}
```

**Go** (middleware + manual spans):

```go
// Initialize once at startup
func initTracer(ctx context.Context) (*sdktrace.TracerProvider, error) {
    endpoint := os.Getenv("OTEL_EXPORTER_OTLP_ENDPOINT")
  
    exporter, err := otlptracegrpc.New(ctx,
        otlptracegrpc.WithEndpoint(endpoint),
        otlptracegrpc.WithInsecure(),
    )
  
    tp := sdktrace.NewTracerProvider(
        sdktrace.WithBatcher(exporter),
        sdktrace.WithResource(resource.NewWithAttributes(
            semconv.ServiceName("inventory-service"),
        )),
    )
  
    otel.SetTracerProvider(tp)
    return tp, nil
}

// Add middleware - automatically traces all HTTP requests
router := gin.New()
router.Use(otelgin.Middleware("inventory-service"))

// Optional: Add manual spans for detailed operations
func (app *App) getItem(c *gin.Context) {
    ctx, span := app.tracer.Start(c.Request.Context(), "getItem")
    defer span.End()
  
    // Add custom attributes
    span.SetAttributes(attribute.String("item.id", id))
  
    // Your business logic
    item, err := db.QueryRow(ctx, query, id)
    if err != nil {
        span.RecordError(err)
        return
    }
}
```

### Context Propagation

This is the **magic** that makes distributed tracing work:

```
Client Request
    â†“
Service A (Python User Service)
    â”‚
    â”œâ”€ Creates Trace ID: xyz789
    â”œâ”€ Creates Span ID: span001
    â”‚
    â”œâ”€ HTTP Request to Service B
    â”‚  Headers:
    â”‚    traceparent: 00-xyz789-span001-01
    â”‚
    â†“
Service B (Rust Order Service)
    â”‚
    â”œâ”€ Extracts trace context from headers
    â”œâ”€ Continues same Trace ID: xyz789
    â”œâ”€ Creates new Span ID: span002
    â”œâ”€ Sets parent: span001
    â”‚
    â””â”€ Both services export to Tempo
  
Result: Single trace with linked spans!
```

### Key Characteristics

- âœ… **PUSH model**: Services push spans to Tempo
- âœ… **OTLP protocol**: Standard OpenTelemetry protocol (gRPC or HTTP)
- âœ… **Batching**: SDK batches spans for efficiency (reduces network calls)
- âœ… **Asynchronous**: Export happens in background, doesn't block app
- âœ… **Sampling**: Only trace a % of requests (configurable)
- âœ… **Context propagation**: Trace IDs flow across service boundaries
- âœ… **Automatic instrumentation**: HTTP, DB, and framework spans created automatically

### Trace Structure in Tempo

```json
{
  "traceID": "xyz789",
  "spans": [
    {
      "spanID": "span001",
      "parentSpanID": null,
      "operationName": "POST /api/users",
      "serviceName": "user-service",
      "startTime": 1702987845000000,
      "duration": 120000,
      "tags": {
        "http.method": "POST",
        "http.status_code": 201,
        "user.email": "user@example.com"
      }
    },
    {
      "spanID": "span002",
      "parentSpanID": "span001",
      "operationName": "INSERT users",
      "serviceName": "user-service",
      "startTime": 1702987845050000,
      "duration": 45000,
      "tags": {
        "db.system": "postgresql",
        "db.statement": "INSERT INTO users..."
      }
    }
  ]
}
```

---

## Why Separate Collectors?

Each telemetry type has fundamentally different characteristics:

| Aspect                  | Metrics                  | Logs               | Traces              |
| ----------------------- | ------------------------ | ------------------ | ------------------- |
| **Volume**        | Low (aggregated)         | High (every event) | Medium (sampled)    |
| **Collection**    | PULL (scrape)            | PUSH (stream)      | PUSH (export)       |
| **Storage**       | Time-series DB           | Compressed text    | Span database       |
| **Query Pattern** | Aggregations             | Text search        | Trace ID lookup     |
| **Retention**     | Weeks/Months             | Days/Week          | Hours/Days          |
| **Cardinality**   | Low (10-100 per service) | Unlimited          | Medium              |
| **Index**         | All data                 | Labels only        | Trace IDs           |
| **Cost**          | Low                      | Medium             | Low (with sampling) |

### Benefits of Separate Systems

1. **Optimized Storage**

   - Prometheus: Efficient time-series compression
   - Loki: Logs stored in chunks, only labels indexed
   - Tempo: Trace-oriented storage with trace ID index
2. **Independent Scaling**

   - Scale metrics storage separately from log storage
   - High log volume doesn't affect trace performance
   - Different retention policies per system
3. **Specialized Query Languages**

   - **PromQL** for metrics: `rate(http_requests_total[5m])`
   - **LogQL** for logs: `{app="user-service"} |= "error"`
   - **TraceQL** for traces: `{service.name="user-service" && http.status_code=500}`
4. **Collection Method Matches Data Type**

   - Metrics: PULL works well (services expose state)
   - Logs: PUSH works well (continuous stream)
   - Traces: PUSH works well (spans need to be sent together)

---

## The Magic: Correlation in Grafana

Even though data flows through separate pipelines, Grafana **unifies** them:

### Example Workflow

```
1. View Metrics Dashboard
   â””â”€ See spike in error rate (Prometheus)
   
2. Click "View Related Logs"
   â””â”€ Grafana queries Loki for same time range
   â””â”€ Filters by same service labels
   
3. See Error Logs
   â””â”€ Logs show trace_id: "abc123"
   
4. Click trace_id Link
   â””â”€ Grafana queries Tempo for trace "abc123"
   
5. View Complete Trace
   â””â”€ See all spans across all services
   â””â”€ Identify exactly where error occurred
```

### How Correlation Works

**1. Trace ID in Logs** - OpenTelemetry automatically injects:

```python
# OpenTelemetry automatically adds trace context to logs
logger.info("User created")
# Output includes: {"msg":"User created","trace_id":"abc123","span_id":"span001"}
```

**2. Grafana Datasource Configuration**:

```yaml
# grafana-datasources.yaml
datasources:
  - name: Loki
    type: loki
    url: http://loki:3100
    jsonData:
      derivedFields:
        - datasourceUid: tempo
          matcherRegex: "traceID=(\\w+)"
          name: TraceID
          url: "$${__value.raw}"
        
  - name: Tempo
    type: tempo
    url: http://tempo:3200
    jsonData:
      tracesToLogs:
        datasourceUid: 'loki'
        tags: ['service_name']
        spanStartTimeShift: '-1h'
        spanEndTimeShift: '1h'
      tracesToMetrics:
        datasourceUid: 'prometheus'
```

**3. Exemplars** (links from metrics to traces):

```python
# Prometheus can store exemplars linking metrics to traces
http_requests_total{method="POST",endpoint="/api/users",status="500"} 1 {trace_id="abc123"}
```

**4. Time Correlation**:

- All systems use same timestamps
- Grafana can show metrics, logs, and traces for same time range
- Automatically correlates by time + service labels

### Visual Correlation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Grafana Unified View                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  [Metrics Dashboard]                                 â”‚
â”‚  http_requests_total{status="500"} = 15 â† Spike!     â”‚
â”‚         â”‚                                            â”‚
â”‚         â”‚ Click "View Related Logs"                  â”‚
â”‚         â–¼                                            â”‚
â”‚  [Logs View]                                         â”‚
â”‚  {app="user-service"} |= "error"                     â”‚
â”‚  2025-12-19 10:30:45 ERROR Database timeout          â”‚
â”‚    trace_id: "abc123" â† Click this                   â”‚
â”‚         â”‚                                            â”‚
â”‚         â”‚ Jump to Trace                              â”‚
â”‚         â–¼                                            â”‚
â”‚  [Trace View]                                        â”‚
â”‚  Trace: abc123                                       â”‚
â”‚  â”œâ”€ POST /api/users (120ms)                          â”‚
â”‚  â”‚  â”œâ”€ INSERT users (45ms)                           â”‚
â”‚  â”‚  â””â”€ PostgreSQL query (80ms) â† ERROR HERE!         â”‚
â”‚  â””â”€ NATS publish (5ms)                               â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Environment Variables for Configuration

All services use standard OpenTelemetry environment variables:

```bash
# Traces
OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4317
OTEL_SERVICE_NAME=my-service
OTEL_TRACES_SAMPLER=parentbased_traceidratio
OTEL_TRACES_SAMPLER_ARG=0.1  # 10% sampling rate

# Logs
LOG_LEVEL=info
LOG_FORMAT=json

# Application-specific
DATABASE_URL=postgresql://user:pass@host:5432/db
NATS_URL=nats://nats:4222
```

**Benefits:**

- âœ… No code changes to modify configuration
- âœ… Same variables across all languages
- âœ… Easy to change per environment (dev, staging, prod)
- âœ… Standard OpenTelemetry convention

---

## Performance and Overhead

### Metrics (Prometheus)

- **Collection overhead**: ~0.5% CPU per scrape
- **Service overhead**: Minimal (counters in memory)
- **Network**: Small (one HTTP GET every 30s)
- **Storage**: ~1-2 GB per day per 100 services

### Logs (Loki)

- **Collection overhead**: ~2-5% CPU (Promtail on node)
- **Service overhead**: None (just stdout)
- **Network**: Medium (continuous stream)
- **Storage**: ~500 MB per day per 100 services

### Traces (Tempo)

- **Collection overhead**: ~1-3% CPU with 10% sampling
- **Service overhead**: Minimal with batching
- **Network**: Low with batching and sampling
- **Storage**: ~2-5 GB per day per 100 services (10% sampling)

### Total Observability Overhead

- **With proper configuration**: < 5% total overhead
- **Key optimizations**:
  - Trace sampling (10% in production)
  - Async export (doesn't block requests)
  - Batching (reduces network calls)
  - Efficient storage (Loki doesn't index content)

---

## Summary

### Three Separate Pipelines

1. **Metrics (Prometheus)**

   - Prometheus **scrapes** `/metrics` endpoints every 30s
   - Services expose metrics in memory
   - Time-series storage optimized for aggregations
   - Query with PromQL
2. **Logs (Loki)**

   - Services write to **stdout/stderr**
   - Promtail DaemonSet **collects** from all pods
   - Ships to Loki with labels
   - Query with LogQL
3. **Traces (Tempo)**

   - Services **export** spans via OTLP (gRPC/HTTP)
   - OpenTelemetry SDK batches and sends
   - Tempo stores and indexes by trace ID
   - Query with TraceQL or trace ID

### Unified in Grafana

Despite separate pipelines, Grafana provides:

- âœ… Single pane of glass
- âœ… Correlation between metrics, logs, traces
- âœ… Jump from metric â†’ log â†’ trace
- âœ… Time-correlated views
- âœ… Service-level correlation
- âœ… Trace ID linking

### Why This Architecture?

- **Specialized storage** for each data type
- **Optimized queries** (PromQL, LogQL, TraceQL)
- **Independent scaling** and retention
- **Cost-effective** (Loki doesn't index content, traces sampled)
- **Standard protocols** (Prometheus, OTLP, etc.)
- **Flexible** (can replace any component)

This is the **modern observability stack** - three specialized collection pipelines unified by Grafana! ğŸš€

---

## Quick Reference

| Data Type         | Collection | Agent    | Protocol  | Port      | Storage    | Query   |
| ----------------- | ---------- | -------- | --------- | --------- | ---------- | ------- |
| **Metrics** | PULL       | None     | HTTP      | 8000-8002 | Prometheus | PromQL  |
| **Logs**    | PUSH       | Promtail | HTTP      | 3100      | Loki       | LogQL   |
| **Traces**  | PUSH       | None     | OTLP/gRPC | 4317      | Tempo      | TraceQL |

All queryable from **Grafana** on port **3000**.
